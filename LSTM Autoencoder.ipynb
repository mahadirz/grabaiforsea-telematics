{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Autoencoder\n",
    "* Used for feature extractor to feed next model that has fixed length of input.\n",
    "\n",
    "## Motivation\n",
    "* Training data using human intervention for feature I believe is not something scalable. If we look into image, initially most classification with high accuracy came from feature built by hand or from expert intuition but look at now mostly deep learning outperform traditional machine learning at image classification\n",
    "* To have ability to learn from full example of dataset \n",
    "\n",
    "## TODO\n",
    "* Use variable length of input without padding and truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'labels', 'data_dictionary.xlsx', 'features']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import h5py\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gc\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"safety\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "# lstm autoencoder recreate sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Masking\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 689382141307509918\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11286285517\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13583097981217109386\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 7} ) \n",
    "sess = tf.Session(config=config) \n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#booking_ids = np.load('booking_ids.npy')\n",
    "#Xpad = np.load('Xpad.npy')\n",
    "# read back\n",
    "with h5py.File('data/Xpad.h5', 'r') as hf:\n",
    "    Xpad = hf['Xpad'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "codeCollapsed": false,
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "# Taken from https://stackoverflow.com/a/55264099\n",
    "def FindBatchSize(model):\n",
    "    \"\"\"#model: model architecture, that is yet to be trained\"\"\"\n",
    "    import os, sys, psutil, gc, tensorflow, keras\n",
    "    import numpy as np\n",
    "    from keras import backend as K\n",
    "    BatchFound= 16\n",
    "\n",
    "    try:\n",
    "        total_params= int(model.count_params());    GCPU= \"CPU\"\n",
    "        #find whether gpu is available\n",
    "        try:\n",
    "            if K.tensorflow_backend._get_available_gpus()== []:\n",
    "                GCPU= \"CPU\";    #CPU and Cuda9GPU\n",
    "            else:\n",
    "                GCPU= \"GPU\"\n",
    "        except:\n",
    "            from tensorflow.python.client import device_lib;    #Cuda8GPU\n",
    "            def get_available_gpus():\n",
    "                local_device_protos= device_lib.list_local_devices()\n",
    "                return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "            if \"gpu\" not in str(get_available_gpus()).lower():\n",
    "                GCPU= \"CPU\"\n",
    "            else:\n",
    "                GCPU= \"GPU\"\n",
    "\n",
    "        #decide batch size on the basis of GPU availability and model complexity\n",
    "        if (GCPU== \"GPU\") and (os.cpu_count() >15) and (total_params <1000000):\n",
    "            BatchFound= 64    \n",
    "        if (os.cpu_count() <16) and (total_params <500000):\n",
    "            BatchFound= 64  \n",
    "        if (GCPU== \"GPU\") and (os.cpu_count() >15) and (total_params <2000000) and (total_params >=1000000):\n",
    "            BatchFound= 32      \n",
    "        if (GCPU== \"GPU\") and (os.cpu_count() >15) and (total_params >=2000000) and (total_params <10000000):\n",
    "            BatchFound= 16  \n",
    "        if (GCPU== \"GPU\") and (os.cpu_count() >15) and (total_params >=10000000):\n",
    "            BatchFound= 8       \n",
    "        if (os.cpu_count() <16) and (total_params >5000000):\n",
    "            BatchFound= 8    \n",
    "        if total_params >100000000:\n",
    "            BatchFound= 1\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "\n",
    "        #find percentage of memory used\n",
    "        memoryused= psutil.virtual_memory()\n",
    "        memoryused= float(str(memoryused).replace(\" \", \"\").split(\"percent=\")[1].split(\",\")[0])\n",
    "        if memoryused >75.0:\n",
    "            BatchFound= 8\n",
    "        if memoryused >85.0:\n",
    "            BatchFound= 4\n",
    "        if memoryused >90.0:\n",
    "            BatchFound= 2\n",
    "        if total_params >100000000:\n",
    "            BatchFound= 1\n",
    "        print(\"Batch Size:  \"+ str(BatchFound));    gc.collect()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    memoryused= [];    total_params= [];    GCPU= \"\";\n",
    "    del memoryused, total_params, GCPU;    gc.collect()\n",
    "    return BatchFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding value\n",
    "pad_value = 2\n",
    "max_seq_len = Xpad.shape[1]\n",
    "dimension = Xpad.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The EarlyStopping callback monitors training accuracy:\n",
    "# if it fails to improve for two consecutive epochs,\n",
    "# training stops early\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(\n",
    "        filepath='models/lstm-autoencoder-best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        monitor='val_loss', save_best_only=True, save_weights_only=False),\n",
    "    EarlyStopping(monitor='val_loss', patience=1, mode='min')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_9 (Masking)          (None, 1000, 11)          0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 100)               44800     \n",
      "_________________________________________________________________\n",
      "repeat_vector_11 (RepeatVect (None, 1000, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 1000, 100)         80400     \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 1000, 11)          1111      \n",
      "=================================================================\n",
      "Total params: 126,311\n",
      "Trainable params: 126,311\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# autoencoder\n",
    "# A bit messy here, tuning the hyperparamter by hand for now\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=pad_value, input_shape=(max_seq_len, dimension)))\n",
    "#model.add(LSTM(100, activation='relu', input_shape=(max_seq_len,dimension), return_sequences=False))\n",
    "#model.add(LSTM(256, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=False))\n",
    "model.add(RepeatVector(max_seq_len))\n",
    "#model.add(LSTM(256, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(dimension)))\n",
    "opt = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=1.)\n",
    "#opt = optimizers.Adam(lr=0.01, clipnorm=1.)\n",
    "model.compile(optimizer=opt, loss='mean_absolute_error', metrics=['mse'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  64\n"
     ]
    }
   ],
   "source": [
    "batch_size = FindBatchSize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19863, 1000, 11)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xpad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15890 samples, validate on 3973 samples\n",
      "Epoch 1/10\n",
      "15890/15890 [==============================] - 105s 7ms/step - loss: 1.0251 - mean_squared_error: 1.6987 - val_loss: 0.9416 - val_mean_squared_error: 1.5160\n",
      "Epoch 2/10\n",
      "15890/15890 [==============================] - 102s 6ms/step - loss: 0.8337 - mean_squared_error: 1.2921 - val_loss: 0.7241 - val_mean_squared_error: 1.0103\n",
      "Epoch 3/10\n",
      "15890/15890 [==============================] - 102s 6ms/step - loss: 0.7235 - mean_squared_error: 0.8956 - val_loss: 0.7233 - val_mean_squared_error: 0.8672\n",
      "Epoch 4/10\n",
      "15890/15890 [==============================] - 102s 6ms/step - loss: 0.7168 - mean_squared_error: 0.9115 - val_loss: 0.7140 - val_mean_squared_error: 0.9643\n",
      "Epoch 5/10\n",
      "15890/15890 [==============================] - 101s 6ms/step - loss: 0.7120 - mean_squared_error: 0.9746 - val_loss: 0.7124 - val_mean_squared_error: 0.9792\n",
      "Epoch 6/10\n",
      "15890/15890 [==============================] - 100s 6ms/step - loss: 0.7111 - mean_squared_error: 0.9740 - val_loss: 0.7119 - val_mean_squared_error: 0.9719\n",
      "Epoch 7/10\n",
      "15890/15890 [==============================] - 101s 6ms/step - loss: 0.7108 - mean_squared_error: 0.9692 - val_loss: 0.7118 - val_mean_squared_error: 0.9718\n",
      "Epoch 8/10\n",
      "15890/15890 [==============================] - 101s 6ms/step - loss: 0.7106 - mean_squared_error: 0.9707 - val_loss: 0.7116 - val_mean_squared_error: 0.9718\n",
      "Epoch 9/10\n",
      "15890/15890 [==============================] - 102s 6ms/step - loss: 0.7104 - mean_squared_error: 0.9688 - val_loss: 0.7114 - val_mean_squared_error: 0.9711\n",
      "Epoch 10/10\n",
      "15890/15890 [==============================] - 104s 7ms/step - loss: 0.7102 - mean_squared_error: 0.9700 - val_loss: 0.7112 - val_mean_squared_error: 0.9703\n",
      "CPU times: user 33min 27s, sys: 7min 25s, total: 40min 53s\n",
      "Wall time: 17min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0072860470>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# fit model\n",
    "# used bigger batch size and small epochs due to time constraint\n",
    "# encountered NaN issue has been debugging for hours, ref https://towardsdatascience.com/debugging-a-machine-learning-model-written-in-tensorflow-and-keras-f514008ce736\n",
    "model.fit(Xpad, Xpad, epochs=10, verbose=1, batch_size=1000, callbacks=callbacks_list,\n",
    "                      validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val': '0.71', 'epoch': '04', 'file': 'models/lstm-autoencoder-best_model.04-0.71.h5'}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve best model\n",
    "def get_best_model(path, file_prefix=\"\", mode='min'):\n",
    "  files = os.listdir(path)\n",
    "  data = []\n",
    "  best = 0\n",
    "  for i,file in enumerate(files):\n",
    "    r = re.search(file_prefix+\"\\.(\\d+?)-([\\d.]+?)\\.h5\", file, re.IGNORECASE)\n",
    "    if r:\n",
    "        data.append({'file':os.path.join(path, file), 'epoch': r.group(1), 'val':r.group(2)})\n",
    "        if mode == 'min' and data[best]['val'] > data[-1]['val']:\n",
    "          best = i\n",
    "        elif mode == 'max' and data[best]['val'] < data[-1]['val']:\n",
    "          best = i\n",
    "    return data[best]\n",
    "            \n",
    "  \n",
    "best_model = get_best_model(\"models\", \"lstm-autoencoder-best_model\")\n",
    "model1 = load_model(best_model['file'])\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_9_input (InputLayer) (None, 1000, 11)          0         \n",
      "_________________________________________________________________\n",
      "masking_9 (Masking)          (None, 1000, 11)          0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 100)               44800     \n",
      "=================================================================\n",
      "Total params: 44,800\n",
      "Trainable params: 44,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "(19863, 100)\n",
      "CPU times: user 18min 13s, sys: 4min 8s, total: 22min 22s\n",
      "Wall time: 9min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# connect the encoder LSTM as the output layer\n",
    "model2 = Model(inputs=model1.inputs, outputs=model1.layers[1].output)\n",
    "#plot_model(model, show_shapes=True, to_file='lstm_encoder.png')\n",
    "# get the feature vector for the input sequence\n",
    "print(model2.summary())\n",
    "yhat = model2.predict(Xpad)\n",
    "print(yhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.18 s, sys: 351 ms, total: 2.53 s\n",
      "Wall time: 1.36 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 8.7156408e-02,\n",
       "        8.0527468e-03, 4.7768103e-03, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 7.1555769e-06, 2.9005981e-03, 1.6591525e-01,\n",
       "        9.1693051e-02, 6.0353428e-02, 6.8075177e-12, 2.4456780e-02,\n",
       "        8.9798167e-02, 3.8826701e-04, 0.0000000e+00, 1.1437282e-03,\n",
       "        5.5426961e-01, 1.1586640e-01, 2.5789881e-01, 1.4813118e-01,\n",
       "        0.0000000e+00, 6.3901901e-02, 1.4552786e-03, 4.0595583e-03,\n",
       "        2.8994054e-02, 0.0000000e+00, 4.2283259e-02, 1.1469223e-02,\n",
       "        5.2422974e-02, 1.6570378e-02, 2.9908579e-06, 2.1865903e-14,\n",
       "        5.5143732e-07, 3.0423421e-02, 0.0000000e+00, 1.6376851e-03,\n",
       "        1.1907379e-01, 2.5800967e-01, 6.9169626e-02, 3.4278724e-03,\n",
       "        2.4409366e-01, 1.1782951e-01, 4.6075750e-02, 2.2797266e-01,\n",
       "        9.4397262e-02, 3.3995082e-05, 1.0834193e-03, 1.7993878e-01,\n",
       "        1.3978146e-01, 3.5642271e-04, 2.4428770e-01, 0.0000000e+00,\n",
       "        1.4044857e-01, 4.2119850e-03, 1.2152109e-03, 1.1706369e-01,\n",
       "        6.8626560e-10, 2.6913992e-01, 1.3328952e-01, 0.0000000e+00,\n",
       "        2.5205499e-01, 1.4648931e-01, 3.9818343e-02, 1.4036104e-01,\n",
       "        0.0000000e+00, 0.0000000e+00, 7.0073097e-03, 0.0000000e+00,\n",
       "        2.3691346e-01, 1.0239372e-03, 3.5267639e-01, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.5540720e-02, 7.6338984e-02, 0.0000000e+00,\n",
       "        1.5618631e-01, 4.1564824e-03, 2.9882066e-02, 2.2029908e-02,\n",
       "        2.4019127e-01, 5.9177735e-05, 0.0000000e+00, 1.2770067e-01,\n",
       "        1.0357129e-01, 2.9473359e-02, 0.0000000e+00, 0.0000000e+00,\n",
       "        3.3339567e-02, 0.0000000e+00, 5.0894208e-02, 6.8867140e-02,\n",
       "        1.7515489e-01, 2.6255650e-02, 1.3265698e-03, 4.9588117e-03]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# predict single booking id\n",
    "model2.predict(Xpad[0].reshape(1,1000,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "for i in range(100):\n",
    "  cols.append(\"feat_\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can be used in next model\n",
    "pd.DataFrame(yhat, columns=cols).to_parquet(\"data/features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
